<!DOCTYPE html>
<html>
<head>
  <meta charset='UTF-8'>
  <title>AutoML Final Report</title>
  <style>
body {
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
  line-height: 1.6;
  color: #333;
  max-width: 1200px;
  margin: 0 auto;
  padding: 20px;
  background-color: #f5f5f5;
}
h1 { color: #1976d2; border-bottom: 3px solid #1976d2; padding-bottom: 10px; }
h2 { color: #1976d2; margin-top: 30px; }
h3 { color: #424242; }
table {
  width: 100%;
  border-collapse: collapse;
  margin: 15px 0;
  background-color: white;
}
th, td {
  border: 1px solid #ddd;
  padding: 12px;
  text-align: left;
}
th { background-color: #1976d2; color: white; }
tr:nth-child(even) { background-color: #f9f9f9; }
tr:hover { background-color: #f0f0f0; }
ul { margin: 10px 0; }
li { margin: 5px 0; }
code { background-color: #f4f4f4; padding: 2px 6px; border-radius: 3px; }
strong { color: #1976d2; }
  </style>
<style>@media print { body { margin: 0; } .page-break { page-break-after: always; } }</style></head>
<body>
<p># AutoML Final Report</h2></p>

<p>Generated: 2025-12-20 10:21:12</p>

<h2>1. Dataset Overview

<strong>Original Dataset:<strong>
<ul>
<li>Rows: 1025
<li>Columns: 14
<li>Missing Values: 0 (0.00%)
<li>Duplicate Rows: 723
</ul>

<strong>Column Summary:<strong>
<ul>
<li>Numeric Columns (14): age, sex, cp, trestbps, chol
</ul>
<p>  and 9 more</p>
<ul>
<li>Categorical Columns (0): 
</ul>

<strong>After Preprocessing:<strong>
<ul>
<li>Rows: 1025
<li>Columns: 14
<li>Rows Removed: 0
</ul>


<h2>2. EDA Findings

<strong>Numeric Features Statistics:<strong>

<table>
<tr><td>Feature</td><td>Mean</td><td>Median</td><td>Std</td><td>Min</td><td>Max</td></tr>
<tr><td>age</td><td>54.43</td><td>56.00</td><td>9.07</td><td>29.00</td><td>77.00</td></tr>
<tr><td>sex</td><td>0.70</td><td>1.00</td><td>0.46</td><td>0.00</td><td>1.00</td></tr>
<tr><td>cp</td><td>0.94</td><td>1.00</td><td>1.03</td><td>0.00</td><td>3.00</td></tr>
<tr><td>trestbps</td><td>131.61</td><td>130.00</td><td>17.52</td><td>94.00</td><td>200.00</td></tr>
<tr><td>chol</td><td>246.00</td><td>240.00</td><td>51.59</td><td>126.00</td><td>564.00</td></tr>
<tr><td>fbs</td><td>0.15</td><td>0.00</td><td>0.36</td><td>0.00</td><td>1.00</td></tr>
<tr><td>restecg</td><td>0.53</td><td>1.00</td><td>0.53</td><td>0.00</td><td>2.00</td></tr>
<tr><td>thalach</td><td>149.11</td><td>152.00</td><td>23.01</td><td>71.00</td><td>202.00</td></tr>
<tr><td>exang</td><td>0.34</td><td>0.00</td><td>0.47</td><td>0.00</td><td>1.00</td></tr>
<tr><td>oldpeak</td><td>1.07</td><td>0.80</td><td>1.18</td><td>0.00</td><td>6.20</td></tr>
</table>




<h2>3. Data Quality Issues Detected

<strong>Issues Summary:<strong>
<ul>
<li>High Severity: 3
<li>Medium Severity: 2
<li>Low Severity: 3
</ul>

<strong>Issues Detected:<strong>

<ul>
<li>[HIGH] DUPLICATE_ROWS in 'N/A': 723 (70.54%)
<li>[MEDIUM] OUTLIERS in 'trestbps': 30 (2.93%)
<li>[MEDIUM] OUTLIERS in 'chol': 16 (1.56%)
<li>[HIGH] OUTLIERS in 'fbs': 153 (14.93%)
<li>[LOW] OUTLIERS in 'thalach': 4 (0.39%)
<li>[LOW] OUTLIERS in 'oldpeak': 7 (0.68%)
<li>[HIGH] OUTLIERS in 'ca': 87 (8.49%)
<li>[LOW] OUTLIERS in 'thal': 7 (0.68%)
</ul>

<strong>Recommendations:<strong>
<ul>
<li>Remove duplicate rows from dataset
<li>Consider removing or capping outliers using IQR method
</ul>


<h2>4. Preprocessing Decisions

<strong>Methods Applied:<strong>
<ul>
<li>Missing Values Strategy: Median
<li>Outlier Handling: Remove
<li>Scaling Method: Standard
<li>Encoding Method: OneHot
<li>Test Size: 0.2
</ul>

<strong>Impact Summary:<strong>
<ul>
<li>Rows Removed: 0
<li>Missing Values Reduced: 0 â†’ 0
<li>Features Modified: 14 (from 14)
</ul>


<h2>5. Model Configurations & Hyperparameters

<strong>Training Configuration:<strong>
<ul>
<li>Test Size: 0.2
<li>Random State: 42
<li>Hyperparameter Tuning: Yes
</ul>

<strong>Models Trained:<strong>
<ul>
<li>Logistic Regression
<li>K-Neighbors Classifier
<li>Decision Tree Classifier
<li>Gaussian Naive Bayes
<li>Random Forest
<li>Support Vector Machine
<li>Decision Tree Rule-based
</ul>

<ul>
<li><strong>Logistic Regression (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}</p>

<ul>
<li><strong>K-Neighbors Classifier (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'metric': 'manhattan', 'n_neighbors': 5, 'weights': 'distance'}</p>

<ul>
<li><strong>Decision Tree Classifier (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}</p>

<ul>
<li><strong>Gaussian Naive Bayes (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'var_smoothing': 0.012915496650148827}</p>

<ul>
<li><strong>Random Forest (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}</p>

<ul>
<li><strong>Support Vector Machine (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'C': 100, 'gamma': 'scale', 'kernel': 'rbf'}</p>

<ul>
<li><strong>Decision Tree Rule-based (Tuned)<strong> (Tuned)
</ul>
<p>  Best Params: {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}</p>


<h2>6. Model Performance Comparison

<table>
<tr><td>Model</td><td>Accuracy</td><td>Precision</td><td>Recall</td><td>F1-Score</td><td>Training Time</td></tr>
<tr><td>Logistic Regression</td><td>0.7951</td><td>0.8023</td><td>0.7951</td><td>0.7938</td><td>0.01s</td></tr>
<tr><td>K-Neighbors Classifier</td><td>0.8341</td><td>0.8387</td><td>0.8341</td><td>0.8335</td><td>0.01s</td></tr>
<tr><td>Decision Tree Classifier</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>0.01s</td></tr>
<tr><td>Gaussian Naive Bayes</td><td>0.8000</td><td>0.8105</td><td>0.8000</td><td>0.7982</td><td>0.00s</td></tr>
<tr><td>Random Forest</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>0.36s</td></tr>
<tr><td>Support Vector Machine</td><td>0.8878</td><td>0.8923</td><td>0.8878</td><td>0.8875</td><td>0.23s</td></tr>
<tr><td>Decision Tree Rule-based</td><td>0.8439</td><td>0.8604</td><td>0.8439</td><td>0.8420</td><td>0.01s</td></tr>
<tr><td>Logistic Regression (Tuned)</td><td>0.7951</td><td>0.8023</td><td>0.7951</td><td>0.7938</td><td>26.84s</td></tr>
<tr><td>K-Neighbors Classifier (Tuned)</td><td>1.0000</td><td>1.0000</td><td>1.0000</td><td>1.0000</td><td>1.51s</td></tr>
<tr><td>Decision Tree Classifier (Tuned)</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>2.32s</td></tr>
<tr><td>Gaussian Naive Bayes (Tuned)</td><td>0.8000</td><td>0.8105</td><td>0.8000</td><td>0.7982</td><td>0.28s</td></tr>
<tr><td>Random Forest (Tuned)</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>153.91s</td></tr>
<tr><td>Support Vector Machine (Tuned)</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>18.65s</td></tr>
<tr><td>Decision Tree Rule-based (Tuned)</td><td>0.9854</td><td>0.9858</td><td>0.9854</td><td>0.9854</td><td>2.17s</td></tr>
</table>


<h2>7. Best Model Summary & Justification

<strong>Selected Model: K-Neighbors Classifier (Tuned)<strong>

<strong>Reason: Best F1 score: 1.0000<strong>


<strong>Performance Metrics:<strong>
<ul>
<li>Accuracy: 1.0
<li>Precision: 1.0
<li>Recall: 1.0
<li>F1-Score: 1.0
<li>ROC-AUC: 1.0
<li>Training Time: 1.5126826763153076s
</ul>

<strong>Hyperparameters:<strong>
<ul>
<li>metric: manhattan
<li>n_neighbors: 5
<li>weights: distance
</ul>


</body>
</html>